{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208ee6a9-c3d5-4d5b-9102-33b6cab06aa6",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "# Intel&reg; Gaudi&reg; 2 Model Profiling and Optimization using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14d34d-3cbe-4b29-bf7a-a5f7d6eb22b5",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This tutorial will show the user how to run the Intel Gaudi Profiling tools: the habana_perf_tool and the Tensorboard plug-in on the Intel Gaudi 2 AI Accelerator, and the profiling trace viewer.  These tools will provide the user valueable optimization tips and information to modify any model for better performance.   Following these steps and using these tools can help you better understand some of the bottlenecks of your model.  For more information, please refer to the [Profiling](https://docs.habana.ai/en/latest/Profiling/index.html) section of the documentation for info on how to setup the profiler and the [Optimization Guide](https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/index.html) for additional background on other optimization techniques.\n",
    "\n",
    "| Task                                 | Description                                             | Details                                         |\n",
    "|--------------------------------------|---------------------------------------------------------|-------------------------------------------------|\n",
    "| PyTorch Profiling with TensorBoard   | Obtains Gaudi-specific recommendations for performance using TensorBoard. | [Profiling with PyTorch](https://docs.habana.ai/en/latest/Profiling/Profiling_with_PyTorch.html#profiling-with-pytorch)        |\n",
    "| Review the PT_HPU_METRICS_FILE      | Looks for excessive re-compilations during runtime.     | [Runtime Environment Variables](https://docs.habana.ai/en/latest/PyTorch/Reference/Runtime_Flags.html#pytorch-runtime-flags)                   |                         \n",
    "| Profiling Trace Viewer               | Uses Perfetto to view traces.           |  [Getting Started with Intel Gaudi Profiler](https://docs.habana.ai/en/latest/Profiling/Intel_Gaudi_Profiling/Getting_Started_with_Profiler.html#getting-started-with-profiler)                      |                         \n",
    "| Model Logging                        | Sets ENABLE_CONSOLE to set Logging for debug and analysis. | [Runtime Environment Variables](https://docs.habana.ai/en/latest/PyTorch/Reference/Runtime_Flags.html#pytorch-runtime-flags)                |                         \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50df7f-7e0f-44cd-a4e4-32d5d1219dd0",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "To run the this jupyter notebook and the Tensorboard viewer, set the appropriate ports for access when you ssh into the Intel Gaudi 2 node. you need to ensure that the following ports are open:\n",
    "* 8888 (for running this jupyter notebook)\n",
    "* 6006 (for running Tensorboard)    \n",
    "\n",
    "Do to this, you need to add the following in your overall ssh commmand when connecting to the Intel Gaudi Node:\n",
    "\n",
    "`ssh -L 8888:localhost:8888 -L 6006:localhost:6006 .... `\n",
    "\n",
    "We start with an Intel Gaudi PyTorch Docker image and run this notebook.   For this example, we'll be using the [Swin Transformer](https://huggingface.co/microsoft/swin-base-patch4-window7-224-in22k) model from the Hugging Face Repository running on Hugging Face's Optimum-Habana library.  So the first step is to load the Optimum-Habana library and model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2d3680-76ea-4945-853e-dea9011033f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/demo/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demo/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/demo/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pickleshare in /home/demo/.local/lib/python3.10/site-packages (0.7.5)\n",
      "Requirement already satisfied: ipython in /home/demo/.local/lib/python3.10/site-packages (8.26.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/demo/.local/lib/python3.10/site-packages (from ipython) (3.0.47)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/demo/.local/lib/python3.10/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: decorator in /home/demo/.local/lib/python3.10/site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/demo/.local/lib/python3.10/site-packages (from ipython) (1.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/demo/.local/lib/python3.10/site-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /home/demo/.local/lib/python3.10/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/demo/.local/lib/python3.10/site-packages (from ipython) (2.18.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython) (4.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /home/demo/.local/lib/python3.10/site-packages (from ipython) (4.12.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/demo/.local/lib/python3.10/site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/demo/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in /home/demo/.local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in /home/demo/.local/lib/python3.10/site-packages (from stack-data->ipython) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/demo/.local/lib/python3.10/site-packages (from stack-data->ipython) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/demo/.local/lib/python3.10/site-packages (from stack-data->ipython) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optimum-habana==1.13.2\n",
      "  Using cached optimum_habana-1.13.2-py3-none-any.whl (565 kB)\n",
      "Collecting sentence-transformers[train]==3.0.1\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Collecting optimum\n",
      "  Using cached optimum-1.22.0-py3-none-any.whl (453 kB)\n",
      "Requirement already satisfied: torch in /home/demo/.local/lib/python3.10/site-packages (from optimum-habana==1.13.2) (2.2.2a0+gitb5d0b9b)\n",
      "Collecting diffusers==0.29.2\n",
      "  Using cached diffusers-0.29.2-py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in /home/demo/.local/lib/python3.10/site-packages (from optimum-habana==1.13.2) (0.23.4)\n",
      "Collecting transformers<4.44.0,>=4.43.0\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "Collecting accelerate<0.34.0,>=0.33.0\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Requirement already satisfied: Pillow in /home/demo/.local/lib/python3.10/site-packages (from diffusers==0.29.2->optimum-habana==1.13.2) (10.4.0)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "Requirement already satisfied: filelock in /home/demo/.local/lib/python3.10/site-packages (from diffusers==0.29.2->optimum-habana==1.13.2) (3.15.4)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers==0.29.2->optimum-habana==1.13.2) (4.6.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/demo/.local/lib/python3.10/site-packages (from diffusers==0.29.2->optimum-habana==1.13.2) (0.4.4)\n",
      "Requirement already satisfied: requests in /home/demo/.local/lib/python3.10/site-packages (from diffusers==0.29.2->optimum-habana==1.13.2) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/demo/.local/lib/python3.10/site-packages (from diffusers==0.29.2->optimum-habana==1.13.2) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /home/demo/.local/lib/python3.10/site-packages (from sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (4.66.4)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/demo/.local/lib/python3.10/site-packages (from accelerate<0.34.0,>=0.33.0->optimum-habana==1.13.2) (24.1)\n",
      "Requirement already satisfied: psutil in /home/demo/.local/lib/python3.10/site-packages (from accelerate<0.34.0,>=0.33.0->optimum-habana==1.13.2) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/demo/.local/lib/python3.10/site-packages (from accelerate<0.34.0,>=0.33.0->optimum-habana==1.13.2) (6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/demo/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->optimum-habana==1.13.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/demo/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->optimum-habana==1.13.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/demo/.local/lib/python3.10/site-packages (from torch->optimum-habana==1.13.2) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch->optimum-habana==1.13.2) (3.0.3)\n",
      "Requirement already satisfied: networkx in /home/demo/.local/lib/python3.10/site-packages (from torch->optimum-habana==1.13.2) (3.3)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers[sentencepiece]<4.45.0,>=4.29 in /home/demo/.local/lib/python3.10/site-packages (from optimum->optimum-habana==1.13.2) (4.38.0)\n",
      "Collecting transformers[sentencepiece]<4.45.0,>=4.29\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.44.1-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /home/demo/.local/lib/python3.10/site-packages (from transformers<4.44.0,>=4.43.0->optimum-habana==1.13.2) (3.20.3)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/demo/.local/lib/python3.10/site-packages (from datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (2.0.1)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /home/demo/.local/lib/python3.10/site-packages (from datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (3.9.5)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/demo/.local/lib/python3.10/site-packages (from requests->diffusers==0.29.2->optimum-habana==1.13.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers==0.29.2->optimum-habana==1.13.2) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers==0.29.2->optimum-habana==1.13.2) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/demo/.local/lib/python3.10/site-packages (from requests->diffusers==0.29.2->optimum-habana==1.13.2) (3.3.2)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/demo/.local/lib/python3.10/site-packages (from sympy->torch->optimum-habana==1.13.2) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (4.0.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/demo/.local/lib/python3.10/site-packages (from pandas->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/demo/.local/lib/python3.10/site-packages (from pandas->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->sentence-transformers[train]==3.0.1->optimum-habana==1.13.2) (1.16.0)\n",
      "Installing collected packages: sentencepiece, xxhash, threadpoolctl, numpy, joblib, humanfriendly, dill, scipy, pyarrow, multiprocess, coloredlogs, tokenizers, scikit-learn, diffusers, accelerate, transformers, datasets, sentence-transformers, optimum, optimum-habana\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.0\n",
      "    Uninstalling transformers-4.38.0:\n",
      "      Successfully uninstalled transformers-4.38.0\n",
      "Successfully installed accelerate-0.33.0 coloredlogs-15.0.1 datasets-3.0.1 diffusers-0.29.2 dill-0.3.8 humanfriendly-10.0 joblib-1.4.2 multiprocess-0.70.16 numpy-1.26.4 optimum-1.22.0 optimum-habana-1.13.2 pyarrow-17.0.0 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.0.1 sentencepiece-0.2.0 threadpoolctl-3.5.0 tokenizers-0.19.1 transformers-4.43.4 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd ~/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization\n",
    "!pip install pickleshare ipython\n",
    "!pip install optimum-habana==1.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e75d5fc-5196-4b80-8fe8-d58b22949fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 18189, done.\u001b[K\n",
      "remote: Counting objects: 100% (1770/1770), done.\u001b[K\n",
      "remote: Compressing objects: 100% (841/841), done.\u001b[K\n",
      "remote: Total 18189 (delta 1181), reused 1293 (delta 820), pack-reused 16419 (from 1)\u001b[K\n",
      "Receiving objects: 100% (18189/18189), 11.85 MiB | 23.70 MiB/s, done.\n",
      "Resolving deltas: 100% (12520/12520), done.\n",
      "/home/demo/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana\n",
      "Note: switching to 'v1.13.2'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 1266993d Release: v1.13.2\n",
      "/home/demo/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/optimum-habana\n",
    "%cd optimum-habana\n",
    "!git checkout v1.13.2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec63423",
   "metadata": {},
   "source": [
    "We now will go into the image-classification task and load the specfic requirements for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d301b6c1-f975-45f6-893c-13885ef9d18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/demo/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/demo/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.2.2a0+gitb5d0b9b)\n",
      "Requirement already satisfied: torchvision>=0.6.0 in /home/demo/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.17.0+b2383d4)\n",
      "Requirement already satisfied: datasets>=2.14.0 in /home/demo/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.0.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/demo/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.5.2)\n",
      "Collecting timm>=0.9.16\n",
      "  Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /home/demo/.local/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/demo/.local/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: sympy in /home/demo/.local/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: filelock in /home/demo/.local/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.15.4)\n",
      "Requirement already satisfied: fsspec in /home/demo/.local/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/demo/.local/lib/python3.10/site-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (10.4.0)\n",
      "Requirement already satisfied: numpy in /home/demo/.local/lib/python3.10/site-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/demo/.local/lib/python3.10/site-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: packaging in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (4.66.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (0.23.4)\n",
      "Requirement already satisfied: pandas in /home/demo/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/demo/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/demo/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/demo/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: safetensors in /home/demo/.local/lib/python3.10/site-packages (from timm>=0.9.16->-r requirements.txt (line 6)) (0.4.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (24.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/demo/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/demo/.local/lib/python3.10/site-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/demo/.local/lib/python3.10/site-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/demo/.local/lib/python3.10/site-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/demo/.local/lib/python3.10/site-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/demo/.local/lib/python3.10/site-packages (from sympy->torch>=1.5.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Installing collected packages: timm, evaluate\n",
      "Successfully installed evaluate-0.4.3 timm-1.0.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/image-classification\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57faeecd-5853-47d7-b148-ab327048447d",
   "metadata": {},
   "source": [
    "### Running the Model\n",
    "Now that the model is loaded, we'll run the model and look for the trace files for analysis. \n",
    "\n",
    "For this model script we can see the profiling set in the utils.py. \n",
    "For other models not in optimum-habana, users can refer to [Profiling_with_PyTorch](https://docs.habana.ai/en/latest/Profiling/Profiling_with_PyTorch.html) to setup profiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48df403-cee4-4898-a4a6-5432d9cc58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   301\t            schedule = torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=1)\n",
      "   302\t            activities = [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.HPU]\n",
      "   303\t\n",
      "   304\t            profiler = torch.profiler.profile(\n",
      "   305\t                schedule=schedule,\n",
      "   306\t                activities=activities,\n",
      "   307\t                on_trace_ready=torch.profiler.tensorboard_trace_handler(output_dir),\n",
      "   308\t                record_shapes=record_shapes,\n",
      "   309\t                with_stack=False,\n",
      "   310\t            )\n",
      "   311\t            self.start = profiler.start\n",
      "   312\t            self.stop = profiler.stop\n",
      "   313\t            self.step = profiler.step\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat -n ../../optimum/habana/utils.py | head -n 313 | tail -n 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a07d2b-f089-435a-b346-50b2eb82b8ca",
   "metadata": {},
   "source": [
    "Run Model to collect trace file (unoptimized)\n",
    "Swin Transformer is a model that capably serves as a general-purpose backbone for computer vision. run_image_classification.py is a script that showcases how to fine-tune Swin Transformer on HPUs.\n",
    "\n",
    "Notice the torch profiler specific commands:\n",
    "\n",
    "- `--profiling_warmup_steps 10` - profiler will wait for warmup steps\n",
    "- `--profiling_steps 3` - records for the next active steps  \n",
    "                             \n",
    "The collected trace files will be saved to ./hpu_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "031de875-5f20-4893-af32-5ce9451afe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING|utils.py:212] 2024-09-28 00:15:35,519 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but habana-frameworks v1.16.2.2 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:225] 2024-09-28 00:15:36,431 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.16.2, this could lead to undefined behavior!\n",
      "/home/demo/.local/lib/python3.10/site-packages/optimum/habana/transformers/training_args.py:366: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/demo/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "gaudi_config.json: 100%|██████████████████████| 90.0/90.0 [00:00<00:00, 315kB/s]\n",
      "/home/demo/.local/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "09/28/2024 00:15:39 - WARNING - __main__ - Process rank: 0, device: hpu, distributed training: False, mixed-precision training: True\n",
      "09/28/2024 00:15:39 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=no,\n",
      "fp8=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/swin,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/outputs/runs/Sep28_00-15-36_hls2-srv02-demolab,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/tmp/outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=3,\n",
      "profiling_warmup_steps=10,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/outputs/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "throughput_warmup_steps=2,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=True,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "README.md: 100%|███████████████████████████| 5.16k/5.16k [00:00<00:00, 15.2MB/s]\n",
      "train-00000-of-00001.parquet: 100%|███████████| 120M/120M [00:01<00:00, 105MB/s]\n",
      "test-00000-of-00001.parquet: 100%|██████████| 23.9M/23.9M [00:00<00:00, 101MB/s]\n",
      "Generating train split: 100%|███| 50000/50000 [00:00<00:00, 94993.82 examples/s]\n",
      "Generating test split: 100%|███| 10000/10000 [00:00<00:00, 100680.13 examples/s]\n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 7.73MB/s]\n",
      "config.json: 100%|█████████████████████████| 1.67M/1.67M [00:00<00:00, 5.04MB/s]\n",
      "[INFO|configuration_utils.py:733] 2024-09-28 00:15:46,866 >> loading configuration file config.json from cache at /home/demo/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-28 00:15:46,877 >> Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": \"0\",\n",
      "    \"automobile\": \"1\",\n",
      "    \"bird\": \"2\",\n",
      "    \"cat\": \"3\",\n",
      "    \"deer\": \"4\",\n",
      "    \"dog\": \"5\",\n",
      "    \"frog\": \"6\",\n",
      "    \"horse\": \"7\",\n",
      "    \"ship\": \"8\",\n",
      "    \"truck\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "model.safetensors: 100%|██████████████████████| 437M/437M [00:03<00:00, 111MB/s]\n",
      "[INFO|modeling_utils.py:3644] 2024-09-28 00:15:51,049 >> loading weights file model.safetensors from cache at /home/demo/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/model.safetensors\n",
      "[INFO|modeling_utils.py:4473] 2024-09-28 00:15:52,012 >> All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:4494] 2024-09-28 00:15:52,013 >> Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "preprocessor_config.json: 100%|████████████████| 255/255 [00:00<00:00, 2.30MB/s]\n",
      "[INFO|image_processing_base.py:375] 2024-09-28 00:15:52,217 >> loading configuration file preprocessor_config.json from cache at /home/demo/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/preprocessor_config.json\n",
      "[INFO|image_processing_utils.py:238] 2024-09-28 00:15:52,219 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_base.py:429] 2024-09-28 00:15:52,219 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 152\n",
      "CPU RAM       : 1056439240 KB\n",
      "------------------------------------------------------------------------------\n",
      "/home/demo/.local/lib/python3.10/site-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-09-28 00:15:55,317] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[INFO|trainer.py:785] 2024-09-28 00:15:55,712 >> ***** Running training *****\n",
      "[INFO|trainer.py:786] 2024-09-28 00:15:55,712 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:787] 2024-09-28 00:15:55,712 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:788] 2024-09-28 00:15:55,712 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:791] 2024-09-28 00:15:55,712 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:792] 2024-09-28 00:15:55,712 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:793] 2024-09-28 00:15:55,712 >>   Total optimization steps = 1,330\n",
      "[INFO|trainer.py:794] 2024-09-28 00:15:55,718 >>   Number of trainable parameters = 86,753,474\n",
      "  1%|▎                                        | 10/1330 [00:29<12:53,  1.71it/s]STAGE:2024-09-28 00:16:25 2152374:2152374 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "  1%|▍                                        | 13/1330 [00:30<07:18,  3.01it/s]STAGE:2024-09-28 00:16:25 2152374:2152374 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-09-28 00:16:25 2152374:2152374 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "{'loss': 0.4454, 'grad_norm': 38.55900573730469, 'learning_rate': 1.8721804511278196e-05, 'epoch': 0.75, 'memory_allocated (GB)': 11.15, 'max_memory_allocated (GB)': 11.53, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 0.2597, 'grad_norm': 33.04595184326172, 'learning_rate': 7.443609022556391e-06, 'epoch': 1.5, 'memory_allocated (GB)': 11.15, 'max_memory_allocated (GB)': 18.39, 'total_memory_available (GB)': 94.62}\n",
      "100%|███████████████████████████████████████| 1330/1330 [03:57<00:00,  7.66it/s][INFO|trainer.py:1079] 2024-09-28 00:19:52,795 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 237.0802, 'train_samples_per_second': 402.469, 'train_steps_per_second': 6.297, 'train_loss': 0.32188967941398905, 'epoch': 2.0, 'memory_allocated (GB)': 11.15, 'max_memory_allocated (GB)': 18.39, 'total_memory_available (GB)': 94.62}\n",
      "100%|███████████████████████████████████████| 1330/1330 [03:57<00:00,  5.61it/s]\n",
      "[INFO|trainer.py:1658] 2024-09-28 00:19:52,801 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:472] 2024-09-28 00:19:53,075 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:2765] 2024-09-28 00:19:53,426 >> Model weights saved in /tmp/outputs/model.safetensors\n",
      "[INFO|image_processing_base.py:258] 2024-09-28 00:19:53,427 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:125] 2024-09-28 00:19:53,427 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "***** train metrics *****\n",
      "  epoch                       =          2.0\n",
      "  max_memory_allocated (GB)   =        18.39\n",
      "  memory_allocated (GB)       =        11.15\n",
      "  total_flos                  = 6211366589GF\n",
      "  total_memory_available (GB) =        94.62\n",
      "  train_loss                  =       0.3219\n",
      "  train_runtime               =   0:03:57.08\n",
      "  train_samples_per_second    =      402.469\n",
      "  train_steps_per_second      =        6.297\n",
      "[INFO|modelcard.py:449] 2024-09-28 00:19:53,612 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}}\n"
     ]
    }
   ],
   "source": [
    "!python3 run_image_classification.py \\\n",
    "    --model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "    --dataset_name cifar10 \\\n",
    "    --output_dir /tmp/outputs/ \\\n",
    "    --remove_unused_columns False \\\n",
    "    --image_column_name img \\\n",
    "    --do_train \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 64 \\\n",
    "    --evaluation_strategy no \\\n",
    "    --save_strategy no \\\n",
    "    --load_best_model_at_end False \\\n",
    "    --save_total_limit 3 \\\n",
    "    --seed 1337 \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --use_hpu_graphs_for_training \\\n",
    "    --gaudi_config_name Habana/swin \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --bf16 \\\n",
    "    --report_to none \\\n",
    "    --throughput_warmup_steps 2 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ignore_mismatched_sizes \\\n",
    "    --profiling_warmup_steps 10 \\\n",
    "    --profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f23106fe-e911-4e67-9c8f-4ad8d587408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/demo/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification/hpu_profile\n",
      "total 183168\n",
      "drwxrwxr-x 2 demo demo      4096 Sep 28 00:16 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxrwxr-x 4 demo demo      4096 Sep 28 00:16 \u001b[01;34m..\u001b[0m/\n",
      "-rw-rw-r-- 1 demo demo 187553452 Sep 28 00:16 hls2-srv02-demolab_2152374.1727482606813402488.pt.trace.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demo/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd hpu_profile\n",
    "%ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21347a9a",
   "metadata": {},
   "source": [
    "### Reviewing the Details in Tensorboard and perf_tool\n",
    "Now that the training is completed, you can see the trace files (...pt.trace.json) have been generated and now can be viewed.  Two types of information are produced by TensorBoard:\n",
    "\n",
    "Model Performance Tracking - While your workload is being processed in batches, you can track the progress of the training process on the dashboard in real-time by monitoring the model’s cost (loss) and accuracy.\n",
    "\n",
    "Profiling Analysis - Right after the last requested step was completed, the collected profiling data is analyzed by TensorBoard and then immediately submitted to your browser, without any need to wait till the training process is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8d01f1-1c80-4ba7-8384-2626e93f6ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9f673ee06fca407\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9f673ee06fca407\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=/home/demo/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification/hpu_profile --port 6006    # Your port selection may vary, default is 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6df7a-d03d-4994-b8c5-ac9099b68567",
   "metadata": {},
   "source": [
    "If you do not want to run the TensorBoard UI, you can take the same .json log files and use the habana_perf_tool that will parse the existing .json file and provide the same recommendations for performance enhancements, but in a text form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9dee212-948f-473c-a4d8-b530525071cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-28 00:24:19,779 - pytorch_profiler - DEBUG - Loading /home/demo/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification/hpu_profile/hls2-srv02-demolab_2152374.1727482606813402488.pt.trace.json\n",
      "Import Data (KB): 100%|█████████████| 183157/183157 [00:01<00:00, 131841.86it/s]\n",
      "2024-09-28 00:24:22,182 - pytorch_profiler - DEBUG - Please wait for initialization to finish ...\n",
      "2024-09-28 00:24:28,206 - pytorch_profiler - DEBUG - PT Track ids: BridgeTrackIds.Result(pt_bridge_launch='48,34,31', pt_bridge_compute='32', pt_mem_copy='34', pt_mem_log='', pt_build_graph='33,50,51,47')\n",
      "2024-09-28 00:24:28,207 - pytorch_profiler - DEBUG - Track ids: TrackIds.Result(forward='30', backward='46', synapse_launch='0,2,49', synapse_wait='1,36', device_mme='42,43,44,45', device_tpc='6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29', device_dma='5,37,38,39,40,41')\n",
      "2024-09-28 00:24:29,021 - pytorch_profiler - DEBUG - Device ratio: 34.41 % (202.615 ms, 588.88 ms)\n",
      "2024-09-28 00:24:29,021 - pytorch_profiler - DEBUG - Device/Host ratio: 34.41% / 65.59%\n",
      "2024-09-28 00:24:29,401 - pytorch_profiler - DEBUG - Host Summary Graph Build: 12.60 % (72.648976 ms, 576.716 ms)\n",
      "2024-09-28 00:24:29,463 - pytorch_profiler - DEBUG - Host Summary DataLoader: 57.50 % (331.616 ms, 576.716 ms)\n",
      "2024-09-28 00:24:29,586 - pytorch_profiler - DEBUG - Host Summary Input Time: 3.04 % (17.555 ms, 576.716 ms)\n",
      "2024-09-28 00:24:29,661 - pytorch_profiler - DEBUG - Host Summary Compile Time: 0.00 % (0.0 ms, 576.716 ms)\n",
      "2024-09-28 00:24:30,082 - pytorch_profiler - DEBUG - Device Summary MME Lower Precision Ratio: 81.41%\n",
      "2024-09-28 00:24:30,082 - pytorch_profiler - DEBUG - Device Host Overlapping degree: 94.00 %\n",
      "2024-09-28 00:24:30,082 - pytorch_profiler - DEBUG - Host Recommendations: \n",
      "2024-09-28 00:24:30,082 - pytorch_profiler - DEBUG - \tThis run has high time cost on input data loading. 57.50% of the step time is in DataLoader. You could use Habana DataLoader. Or you could try to tune num_workers on DataLoader's construction.\n",
      "2024-09-28 00:24:30,353 - pytorch_profiler - DEBUG - [Device Summary] MME total time 75.01 ms\n",
      "2024-09-28 00:24:35,076 - pytorch_profiler - DEBUG - [Device Summary] MME/TPC overlap time 45.47 ms\n",
      "2024-09-28 00:24:35,077 - pytorch_profiler - DEBUG - [Device Summary] TPC total time 100.56 ms\n",
      "2024-09-28 00:24:36,980 - pytorch_profiler - DEBUG - [Device Summary] DMA total time 26.09 ms\n",
      "2024-09-28 00:24:36,980 - pytorch_profiler - DEBUG - [Device Summary] Idle total time: 0.95 ms\n"
     ]
    }
   ],
   "source": [
    "!habana_perf_tool --trace /home/demo/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification/hpu_profile/hls2-srv02-demolab_2152374.1727482606813402488.pt.trace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5821e9-fe68-4c76-93ef-b3b6c8492a02",
   "metadata": {},
   "source": [
    "### Using the Perfetto Trace Viewer\n",
    "Finally, to view the details of the Intel Gaudi Device itself, you can view the traces in the perfetto trace viewer.  \n",
    "\n",
    "This step requires you to set the `hl-prof-config` settings and the Environment variable `HABANA_PROFILE=1` as shown below, this will generate the .hltv file that can be viewed using https://perfetto.habana.ai.  Since this is using the Gaudi profiler, the runtime profiling commands need to be removed.  At the end of this run, you will see a `my_profiling_session_12345.hltv` file that can be loaded into the Perfetto browser.\n",
    "\n",
    "For More Information to enable your model to use the Habana Perfetto Trace viewer, you can refer to the documentation https://docs.habana.ai/en/latest/Profiling/Intel_Gaudi_Profiling/Getting_Started_with_Profiler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5291e8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/demo/ctao/Gaudi-tutorials/PyTorch\n",
      "Resetting config file /home/demo/.habana/prof_config.json\n",
      "Edited file /home/demo/.habana/prof_config.json\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!hl-prof-config -e off -phase=multi-enq -g 1-20 -s my_profiling_session\n",
    "!export HABANA_PROFILE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd26eb57-a519-4935-b7a5-92a56cdc1093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING|utils.py:212] 2024-09-29 00:13:49,095 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but habana-frameworks v1.16.2.2 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:225] 2024-09-29 00:13:49,983 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.16.2, this could lead to undefined behavior!\n",
      "/home/demo/.local/lib/python3.10/site-packages/optimum/habana/transformers/training_args.py:366: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/demo/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/demo/.local/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "09/29/2024 00:13:52 - WARNING - __main__ - Process rank: 0, device: hpu, distributed training: False, mixed-precision training: True\n",
      "09/29/2024 00:13:52 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=no,\n",
      "fp8=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/swin,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/outputs/runs/Sep29_00-13-50_hls2-srv02-demolab,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/tmp/outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/outputs/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "throughput_warmup_steps=2,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=True,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "[INFO|configuration_utils.py:733] 2024-09-29 00:13:56,745 >> loading configuration file config.json from cache at /home/demo/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-29 00:13:56,758 >> Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": \"0\",\n",
      "    \"automobile\": \"1\",\n",
      "    \"bird\": \"2\",\n",
      "    \"cat\": \"3\",\n",
      "    \"deer\": \"4\",\n",
      "    \"dog\": \"5\",\n",
      "    \"frog\": \"6\",\n",
      "    \"horse\": \"7\",\n",
      "    \"ship\": \"8\",\n",
      "    \"truck\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3644] 2024-09-29 00:13:56,770 >> loading weights file model.safetensors from cache at /home/demo/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/model.safetensors\n",
      "[INFO|modeling_utils.py:4473] 2024-09-29 00:13:57,656 >> All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:4494] 2024-09-29 00:13:57,657 >> Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|image_processing_base.py:375] 2024-09-29 00:13:57,749 >> loading configuration file preprocessor_config.json from cache at /home/demo/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/preprocessor_config.json\n",
      "[INFO|image_processing_utils.py:238] 2024-09-29 00:13:57,750 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_base.py:429] 2024-09-29 00:13:57,750 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 152\n",
      "CPU RAM       : 1056439240 KB\n",
      "------------------------------------------------------------------------------\n",
      "/home/demo/.local/lib/python3.10/site-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-09-29 00:14:00,841] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[INFO|trainer.py:785] 2024-09-29 00:14:01,108 >> ***** Running training *****\n",
      "[INFO|trainer.py:786] 2024-09-29 00:14:01,108 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:787] 2024-09-29 00:14:01,108 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:788] 2024-09-29 00:14:01,108 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:791] 2024-09-29 00:14:01,108 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:792] 2024-09-29 00:14:01,108 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:793] 2024-09-29 00:14:01,108 >>   Total optimization steps = 1,330\n",
      "[INFO|trainer.py:794] 2024-09-29 00:14:01,112 >>   Number of trainable parameters = 86,753,474\n",
      "{'loss': 0.4454, 'grad_norm': 38.55900573730469, 'learning_rate': 1.8721804511278196e-05, 'epoch': 0.75, 'memory_allocated (GB)': 10.39, 'max_memory_allocated (GB)': 10.78, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 0.2597, 'grad_norm': 33.04595184326172, 'learning_rate': 7.443609022556391e-06, 'epoch': 1.5, 'memory_allocated (GB)': 10.4, 'max_memory_allocated (GB)': 10.78, 'total_memory_available (GB)': 94.62}\n",
      "100%|██████████████████████████████████████▉| 1329/1330 [03:59<00:00,  7.72it/s][INFO|trainer.py:1079] 2024-09-29 00:18:00,743 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 239.67, 'train_samples_per_second': 403.355, 'train_steps_per_second': 6.311, 'train_loss': 0.32188967941398905, 'epoch': 2.0, 'memory_allocated (GB)': 10.4, 'max_memory_allocated (GB)': 17.64, 'total_memory_available (GB)': 94.62}\n",
      "100%|███████████████████████████████████████| 1330/1330 [03:59<00:00,  5.55it/s]\n",
      "[INFO|trainer.py:1658] 2024-09-29 00:18:00,785 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:472] 2024-09-29 00:18:01,088 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:2765] 2024-09-29 00:18:01,523 >> Model weights saved in /tmp/outputs/model.safetensors\n",
      "[INFO|image_processing_base.py:258] 2024-09-29 00:18:01,524 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:125] 2024-09-29 00:18:01,524 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "***** train metrics *****\n",
      "  epoch                       =          2.0\n",
      "  max_memory_allocated (GB)   =        17.64\n",
      "  memory_allocated (GB)       =         10.4\n",
      "  total_flos                  = 6211366589GF\n",
      "  total_memory_available (GB) =        94.62\n",
      "  train_loss                  =       0.3219\n",
      "  train_runtime               =   0:03:59.66\n",
      "  train_samples_per_second    =      403.355\n",
      "  train_steps_per_second      =        6.311\n",
      "[INFO|modelcard.py:449] 2024-09-29 00:18:01,676 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}}\n"
     ]
    }
   ],
   "source": [
    "!HABANA_PROFILE=1 python3 /home/demo/ctao/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification/run_image_classification.py \\\n",
    "    --model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "    --dataset_name cifar10 \\\n",
    "    --output_dir /tmp/outputs/ \\\n",
    "    --remove_unused_columns False \\\n",
    "    --image_column_name img \\\n",
    "    --do_train \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 64 \\\n",
    "    --evaluation_strategy no \\\n",
    "    --save_strategy no \\\n",
    "    --load_best_model_at_end False \\\n",
    "    --save_total_limit 3 \\\n",
    "    --seed 1337 \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --report_to none \\\n",
    "    --use_hpu_graphs_for_training \\\n",
    "    --gaudi_config_name Habana/swin \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --bf16 \\\n",
    "    --throughput_warmup_steps 2 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ignore_mismatched_sizes \n",
    "    #--profiling_warmup_steps 10 \\\n",
    "    #--profiling_steps 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
